{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-06T00:31:26.103805Z",
     "start_time": "2025-07-06T00:31:25.971075Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l6/kt268nls2sqcx51m4w_z09pc0000gn/T/ipykernel_61694/4293934290.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(\"AAPL\", start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame shape: (1635, 14)\n",
      "Final dataset: 1441 samples, each of shape (30, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/l6/kt268nls2sqcx51m4w_z09pc0000gn/T/ipykernel_61694/4293934290.py:46: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/var/folders/l6/kt268nls2sqcx51m4w_z09pc0000gn/T/ipykernel_61694/4293934290.py:47: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import ta\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Download and prepare data\n",
    "start_date = \"2019-01-01\"\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "df = yf.download(\"AAPL\", start=start_date, end=end_date)\n",
    "\n",
    "# Flatten column names if they're MultiIndex tuples (e.g., ('Close', 'AAPL'))\n",
    "df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
    "df = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 2: Add engineered + technical features\n",
    "try:\n",
    "    df['Return'] = df['Close'].pct_change()\n",
    "    df['Candle_Body'] = df['Close'] - df['Open']\n",
    "    df['Range'] = df['High'] - df['Low']\n",
    "\n",
    "    df['rsi'] = ta.momentum.RSIIndicator(close=df['Close']).rsi()\n",
    "    df['macd'] = ta.trend.MACD(close=df['Close']).macd_diff()\n",
    "    df['ema_10'] = ta.trend.EMAIndicator(close=df['Close'], window=10).ema_indicator()\n",
    "    df['bb_bbw'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_wband()\n",
    "    df['adx'] = ta.trend.ADXIndicator(high=df['High'], low=df['Low'], close=df['Close']).adx()\n",
    "    df['stoch'] = ta.momentum.StochasticOscillator(high=df['High'], low=df['Low'], close=df['Close']).stoch()\n",
    "\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "except Exception as e:\n",
    "    print(\"Error while adding technical indicators:\", e)\n",
    "\n",
    "print(\"Final DataFrame shape:\", df.shape)\n",
    "\n",
    "# Step 3: Feature scaling\n",
    "features = df.columns.tolist()\n",
    "features.remove('Close')  # Keep 'Close' for labels\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df[features])\n",
    "scaled_close = MinMaxScaler().fit_transform(df[['Close']])\n",
    "scaled_df = np.concatenate([scaled_close, scaled_features], axis=1)\n",
    "\n",
    "# Step 4: Create sequences and labels\n",
    "SEQ_LEN = 30\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(SEQ_LEN, len(scaled_df) - 3):\n",
    "    window = scaled_df[i-SEQ_LEN:i]\n",
    "    future_price = scaled_df[i+3][0]  # 0 = scaled 'Close'\n",
    "    current_price = scaled_df[i][0]\n",
    "    change = future_price - current_price\n",
    "\n",
    "    threshold = 0.002\n",
    "    if change > threshold:\n",
    "        y.append(1)\n",
    "    elif change < -threshold:\n",
    "        y.append(0)\n",
    "    else:\n",
    "        continue  # Skip small/no change\n",
    "\n",
    "    X.append(window)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Final dataset: {X.shape[0]} samples, each of shape {X.shape[1:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1152 | Val: 144 | Test: 145\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Full dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# 80% train, 10% val, 10% test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Loaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-06T00:31:26.104758Z",
     "start_time": "2025-07-06T00:31:26.086665Z"
    }
   },
   "id": "2837e0cf67eb1b42"
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM_CNN_Attention(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden=64, cnn_out=32, attention_dim=32, num_classes=2):\n",
    "        super(LSTM_CNN_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=2 * lstm_hidden, out_channels=cnn_out, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(cnn_out)\n",
    "\n",
    "        self.attn_fc = nn.Linear(cnn_out, attention_dim)\n",
    "        self.attn_vector = nn.Linear(attention_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(cnn_out, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # (B, T, 2*H)\n",
    "\n",
    "        # CNN\n",
    "        cnn_input = lstm_out.permute(0, 2, 1)  # (B, 2H, T)\n",
    "        cnn_out = F.relu(self.bn(self.conv1d(cnn_input)))  # (B, C, T)\n",
    "        cnn_out = cnn_out.permute(0, 2, 1)  # (B, T, C)\n",
    "\n",
    "        # Attention\n",
    "        energy = torch.tanh(self.attn_fc(cnn_out))  # (B, T, A)\n",
    "        attention_weights = F.softmax(self.attn_vector(energy), dim=1)  # (B, T, 1)\n",
    "        context = torch.sum(attention_weights * cnn_out, dim=1)  # (B, C)\n",
    "\n",
    "        # Classification\n",
    "        out = self.dropout(context)\n",
    "        out = self.fc(out)  # (B, 2)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-06T00:31:26.104881Z",
     "start_time": "2025-07-06T00:31:26.093308Z"
    }
   },
   "id": "fd4e82176db1a6fe"
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Train Loss: 12.4622 | Train Acc: 0.5556 | Val Acc: 0.5625 | LR: 0.001000\n",
      "Epoch 02/50 | Train Loss: 12.2519 | Train Acc: 0.5825 | Val Acc: 0.5625 | LR: 0.001000\n",
      "Epoch 03/50 | Train Loss: 12.2876 | Train Acc: 0.5747 | Val Acc: 0.5486 | LR: 0.001000\n",
      "Epoch 04/50 | Train Loss: 12.1538 | Train Acc: 0.5703 | Val Acc: 0.6250 | LR: 0.001000\n",
      "Epoch 05/50 | Train Loss: 12.0421 | Train Acc: 0.5903 | Val Acc: 0.5903 | LR: 0.001000\n",
      "Epoch 06/50 | Train Loss: 12.0015 | Train Acc: 0.6024 | Val Acc: 0.5556 | LR: 0.001000\n",
      "Epoch 07/50 | Train Loss: 12.0734 | Train Acc: 0.5781 | Val Acc: 0.5972 | LR: 0.001000\n",
      "Epoch 08/50 | Train Loss: 11.9101 | Train Acc: 0.6094 | Val Acc: 0.5694 | LR: 0.001000\n",
      "Epoch 09/50 | Train Loss: 12.0186 | Train Acc: 0.5894 | Val Acc: 0.5625 | LR: 0.001000\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Instantiate model\n",
    "model = LSTM_CNN_Attention(input_size=14)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 50\n",
    "best_val_acc = 0\n",
    "patience = 5\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_correct += (preds == y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    scheduler.step()  # ← Step the learning rate here\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02}/{EPOCHS} | Train Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-06T00:31:30.771497Z",
     "start_time": "2025-07-06T00:31:26.101264Z"
    }
   },
   "id": "b1bb2a522cbcfb19"
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6138\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set (no need to load saved model)\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        outputs = model(X_test)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_correct += (preds == y_test).sum().item()\n",
    "        test_total += y_test.size(0)\n",
    "\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-06T00:31:30.809549Z",
     "start_time": "2025-07-06T00:31:30.770334Z"
    }
   },
   "id": "ea9afbe334c9d94b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
