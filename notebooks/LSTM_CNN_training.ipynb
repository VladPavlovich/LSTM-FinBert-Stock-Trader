{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-05T05:52:42.327031Z",
     "start_time": "2025-07-05T05:52:42.217277Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l6/kt268nls2sqcx51m4w_z09pc0000gn/T/ipykernel_61694/4293934290.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(\"AAPL\", start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame shape: (1635, 14)\n",
      "Final dataset: 1441 samples, each of shape (30, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/l6/kt268nls2sqcx51m4w_z09pc0000gn/T/ipykernel_61694/4293934290.py:46: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/var/folders/l6/kt268nls2sqcx51m4w_z09pc0000gn/T/ipykernel_61694/4293934290.py:47: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import ta\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Download and prepare data\n",
    "start_date = \"2019-01-01\"\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "df = yf.download(\"AAPL\", start=start_date, end=end_date)\n",
    "\n",
    "# Flatten column names if they're MultiIndex tuples (e.g., ('Close', 'AAPL'))\n",
    "df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
    "df = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 2: Add engineered + technical features\n",
    "try:\n",
    "    df['Return'] = df['Close'].pct_change()\n",
    "    df['Candle_Body'] = df['Close'] - df['Open']\n",
    "    df['Range'] = df['High'] - df['Low']\n",
    "\n",
    "    df['rsi'] = ta.momentum.RSIIndicator(close=df['Close']).rsi()\n",
    "    df['macd'] = ta.trend.MACD(close=df['Close']).macd_diff()\n",
    "    df['ema_10'] = ta.trend.EMAIndicator(close=df['Close'], window=10).ema_indicator()\n",
    "    df['bb_bbw'] = ta.volatility.BollingerBands(close=df['Close']).bollinger_wband()\n",
    "    df['adx'] = ta.trend.ADXIndicator(high=df['High'], low=df['Low'], close=df['Close']).adx()\n",
    "    df['stoch'] = ta.momentum.StochasticOscillator(high=df['High'], low=df['Low'], close=df['Close']).stoch()\n",
    "\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "except Exception as e:\n",
    "    print(\"Error while adding technical indicators:\", e)\n",
    "\n",
    "print(\"Final DataFrame shape:\", df.shape)\n",
    "\n",
    "# Step 3: Feature scaling\n",
    "features = df.columns.tolist()\n",
    "features.remove('Close')  # Keep 'Close' for labels\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df[features])\n",
    "scaled_close = MinMaxScaler().fit_transform(df[['Close']])\n",
    "scaled_df = np.concatenate([scaled_close, scaled_features], axis=1)\n",
    "\n",
    "# Step 4: Create sequences and labels\n",
    "SEQ_LEN = 30\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(SEQ_LEN, len(scaled_df) - 3):\n",
    "    window = scaled_df[i-SEQ_LEN:i]\n",
    "    future_price = scaled_df[i+3][0]  # 0 = scaled 'Close'\n",
    "    current_price = scaled_df[i][0]\n",
    "    change = future_price - current_price\n",
    "\n",
    "    threshold = 0.002\n",
    "    if change > threshold:\n",
    "        y.append(1)\n",
    "    elif change < -threshold:\n",
    "        y.append(0)\n",
    "    else:\n",
    "        continue  # Skip small/no change\n",
    "\n",
    "    X.append(window)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Final dataset: {X.shape[0]} samples, each of shape {X.shape[1:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1152 | Val: 144 | Test: 145\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Full dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# 80% train, 10% val, 10% test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Loaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-05T05:52:42.327984Z",
     "start_time": "2025-07-05T05:52:42.297776Z"
    }
   },
   "id": "2837e0cf67eb1b42"
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM_CNN_Attention(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden=64, cnn_out=32, attention_dim=32, num_classes=2):\n",
    "        super(LSTM_CNN_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=2 * lstm_hidden, out_channels=cnn_out, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(cnn_out)\n",
    "\n",
    "        self.attn_fc = nn.Linear(cnn_out, attention_dim)\n",
    "        self.attn_vector = nn.Linear(attention_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(cnn_out, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # (B, T, 2*H)\n",
    "\n",
    "        # CNN\n",
    "        cnn_input = lstm_out.permute(0, 2, 1)  # (B, 2H, T)\n",
    "        cnn_out = F.relu(self.bn(self.conv1d(cnn_input)))  # (B, C, T)\n",
    "        cnn_out = cnn_out.permute(0, 2, 1)  # (B, T, C)\n",
    "\n",
    "        # Attention\n",
    "        energy = torch.tanh(self.attn_fc(cnn_out))  # (B, T, A)\n",
    "        attention_weights = F.softmax(self.attn_vector(energy), dim=1)  # (B, T, 1)\n",
    "        context = torch.sum(attention_weights * cnn_out, dim=1)  # (B, C)\n",
    "\n",
    "        # Classification\n",
    "        out = self.dropout(context)\n",
    "        out = self.fc(out)  # (B, 2)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-05T05:52:42.328054Z",
     "start_time": "2025-07-05T05:52:42.305507Z"
    }
   },
   "id": "fd4e82176db1a6fe"
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 12.4605 | Train Acc: 0.5564 | Val Acc: 0.5625\n",
      "Epoch 2/50 | Train Loss: 12.2515 | Train Acc: 0.5833 | Val Acc: 0.5556\n",
      "Epoch 3/50 | Train Loss: 12.2778 | Train Acc: 0.5720 | Val Acc: 0.5625\n",
      "Epoch 4/50 | Train Loss: 12.1429 | Train Acc: 0.5738 | Val Acc: 0.6181\n",
      "Epoch 5/50 | Train Loss: 12.0462 | Train Acc: 0.5929 | Val Acc: 0.5972\n",
      "Epoch 6/50 | Train Loss: 11.9956 | Train Acc: 0.6050 | Val Acc: 0.5625\n",
      "Epoch 7/50 | Train Loss: 12.0721 | Train Acc: 0.5790 | Val Acc: 0.5764\n",
      "Epoch 8/50 | Train Loss: 11.9015 | Train Acc: 0.6050 | Val Acc: 0.5417\n",
      "Epoch 9/50 | Train Loss: 12.0407 | Train Acc: 0.5877 | Val Acc: 0.5694\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Instantiate model\n",
    "model = LSTM_CNN_Attention(input_size=14)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 50\n",
    "best_val_acc = 0\n",
    "patience = 5\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_correct += (preds == y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-05T05:52:47.623960Z",
     "start_time": "2025-07-05T05:52:42.311765Z"
    }
   },
   "id": "b1bb2a522cbcfb19"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6276\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set (no need to load saved model)\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        outputs = model(X_test)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_correct += (preds == y_test).sum().item()\n",
    "        test_total += y_test.size(0)\n",
    "\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-05T05:52:47.671233Z",
     "start_time": "2025-07-05T05:52:47.625337Z"
    }
   },
   "id": "ea9afbe334c9d94b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
